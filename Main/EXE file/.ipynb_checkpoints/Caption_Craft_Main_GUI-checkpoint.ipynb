{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31183de8-2035-4e5c-9123-9026cd504048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\heyia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus \n",
    "import stopwords\n",
    "import string\n",
    "import json\n",
    "from time import time\n",
    "import pickle\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "541b3ea6-e29e-41aa-9aeb-52a644887040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTextFile(path):\n",
    "    with open(path) as f:\n",
    "        captions = f.readlines()\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "739e5f11-27e0-44ec-9d75-821bdff2acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = readTextFile(\"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/Flickr8k_text/Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ea7b28-a531-42c3-a214-3312d9d10fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "first,second = captions[0].split('\\t')\n",
    "#print(first.split(\".\")[0])\n",
    "#print(second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33fb63c6-3aa5-421b-bb4f-547658bd8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions  = {}\n",
    "\n",
    "for x in captions:\n",
    "    first,second = x.split('\\t')\n",
    "    img_name = first.split(\".\")[0]\n",
    "\n",
    "    if descriptions.get(img_name) is None:\n",
    "        descriptions[img_name] = []\n",
    "\n",
    "    \n",
    "    descriptions[img_name].append(second.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1765d1ce-ff39-44df-8df1-892abe3a2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbe7c168-cbe1-4ba4-a76c-94534f9503cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/Flickr8k_Dataset/\"\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = cv2.imread(IMG_PATH+\"1000268201_693b08cb0e.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "#plt.imshow(img)\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d809c26-72bd-4c00-a0c5-b181112f9d49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"[^a-z]+\",\" \",sentence)\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    sentence = [s for s in sentence if len(s)>1]\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6281ba3-2de7-4822-a0c1-2c93dfe4fec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_text(\"A cat is sitting over the house #64.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aefcf1e8-2bd0-4396-b0ff-da20462001f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,caption_list in descriptions.items():\n",
    "    for i in range(len(caption_list)):\n",
    "        caption_list[i] = clean_text(caption_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfb76c90-211a-42b9-b81a-86525b57ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "feacef62-981d-4fc9-822c-0a99b94de6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"descriptions_1.txt\",\"w\") as f:\n",
    "    f.write(str(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6373c1d7-afb7-4038-9c7c-5ca7c1018cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = None\n",
    "with open(\"descriptions_1.txt\",'r') as f:\n",
    "    descriptions = f.read()\n",
    "\n",
    "json_acceptable_string = descriptions.replace(\"'\",\"\\\"\")\n",
    "descriptions = json.loads(json_acceptable_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2823183f-8116-4cf9-9eb1-bee1528ca3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size : 8424\n"
     ]
    }
   ],
   "source": [
    "#vocabula\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "for key in descriptions.keys():\n",
    "    [vocab.update(sentence.split()) for sentence in descriptions[key]]\n",
    "\n",
    "print(\"Vocab Size : %d\"%len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbb1593e-47d8-407b-ad79-6a7293cb2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words 373837\n"
     ]
    }
   ],
   "source": [
    "total_words = []\n",
    "\n",
    "for key in descriptions.keys():\n",
    "    [total_words.append(i) for des in descriptions[key] for i in des.split()]\n",
    "\n",
    "print(\"Total Words %d\"%len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfe94b2a-1d64-4712-9440-d51827a430ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(total_words)\n",
    "freq_cnt = dict(counter)\n",
    "sorted_freq_cnt = sorted(freq_cnt.items(),reverse=True,key=lambda x:x[1])\n",
    "threshold = 10\n",
    "sorted_freq_cnt = [x for x in sorted_freq_cnt if x[1]>threshold]\n",
    "total_words = [x[0] for x in sorted_freq_cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce9022e4-fadd-4fd4-a07b-6f5124e86c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_data = readTextFile(\"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/Flickr8k_text/Flickr_8k.trainImages.txt\")\n",
    "test_file_data = readTextFile(\"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/Flickr8k_text/Flickr_8k.testImages.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cb00121-faab-4eae-8f22-81581606d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [row.split(\".\")[0] for row in train_file_data]\n",
    "test = [row.split(\".\")[0] for row in test_file_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4f5ead8-efcd-4485-99b6-5ef09e7e5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_descriptions = {}\n",
    "\n",
    "for img_id in train:\n",
    "    train_descriptions[img_id] = []\n",
    "    for cap in descriptions[img_id]:\n",
    "        cap_to_append = \"startseq \" + cap + \" endseq\"\n",
    "        train_descriptions[img_id].append(cap_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2edc8abe-3c15-4c75-8436-b30a0226e721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\heyia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\heyia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(weights=\"imagenet\",input_shape=(224,224,3))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "617995ab-b118-44ed-affc-df99ce98f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = Model(model.input,model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cab0281-b18d-4e00-8722-edeba235e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    img = image.load_img(img,target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b906797b-4fe4-4b90-9623-51c173195673",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_img(IMG_PATH+\"1000268201_693b08cb0e.jpg\")\n",
    "#plt.imshow(img[0]/255.0)\n",
    "#plt.axis(\"off\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e66e3923-d61c-4258-8015-10db6a448873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(img):\n",
    "    img = preprocess_img(img)\n",
    "    feature_vector = model_new.predict(img)\n",
    "    \n",
    "    feature_vector = feature_vector.reshape((-1,))\n",
    "    #print(feature_vector.shape)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e597b7a-3133-4ff2-b667-cf540b56cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode_image(IMG_PATH+\"1000268201_693b08cb0e.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ce7b369-8074-4fd6-8186-0cd5d6c4785a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start = time()\\nencoding_train = {}\\n\\nfor ix,img_id in enumerate(train):\\n    img_path = IMG_PATH+\"/\"+img_id+\".jpg\"\\n    encoding_train[img_id] = encode_image(img_path)\\n\\n    if ix%100==0:\\n        print(\"Encoding in Progress Time Step %d \"%ix)\\n\\nend_time = time()\\nprint(\"Total Time Taken :\",end_time-start)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"start = time()\n",
    "encoding_train = {}\n",
    "\n",
    "for ix,img_id in enumerate(train):\n",
    "    img_path = IMG_PATH+\"/\"+img_id+\".jpg\"\n",
    "    encoding_train[img_id] = encode_image(img_path)\n",
    "\n",
    "    if ix%100==0:\n",
    "        print(\"Encoding in Progress Time Step %d \"%ix)\n",
    "\n",
    "end_time = time()\n",
    "print(\"Total Time Taken :\",end_time-start)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59755b72-4243-472a-b06e-1449f55537dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with open(\"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/encoded_train_features.pkl\",\"wb\") as f:\\n    pickle.dump(encoding_train,f)'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with open(\"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/encoded_train_features.pkl\",\"wb\") as f:\n",
    "    pickle.dump(encoding_train,f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a0b11ee-5ec5-4e57-b5dc-f90813226477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start = time()\\nencoding_test = {}\\n\\nfor ix,img_id in enumerate(test):\\n    img_path = IMG_PATH+\"/\"+img_id+\".jpg\"\\n    encoding_test[img_id] = encode_image(img_path)\\n\\n    if ix%100==0:\\n        print(\"Test Encoding in Progress Time Step %d \"%ix)\\n\\nend_time = time()\\nprint(\"Total Time Taken(Test) :\",end_time-start)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"start = time()\n",
    "encoding_test = {}\n",
    "\n",
    "for ix,img_id in enumerate(test):\n",
    "    img_path = IMG_PATH+\"/\"+img_id+\".jpg\"\n",
    "    encoding_test[img_id] = encode_image(img_path)\n",
    "\n",
    "    if ix%100==0:\n",
    "        print(\"Test Encoding in Progress Time Step %d \"%ix)\n",
    "\n",
    "end_time = time()\n",
    "print(\"Total Time Taken(Test) :\",end_time-start)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1165baf0-b2c8-4cdb-baba-75e550e9be13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'with open(\"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/encoded_test_features.pkl\",\"wb\") as f:\\n    pickle.dump(encoding_test,f)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"with open(\"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/encoded_test_features.pkl\",\"wb\") as f:\n",
    "    pickle.dump(encoding_test,f)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35b9d4ac-e310-4f0f-8a44-dfee1868bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_features = \"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/encoded_train_features.pkl\"\n",
    "encoded_test_features = \"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/encoded_test_features.pkl\"\n",
    "\n",
    "with open(encoded_train_features, \"rb\") as f:\n",
    "    encoding_train_file = pickle.load(f)\n",
    "\n",
    "with open(encoded_test_features, \"rb\") as f:\n",
    "    encoding_test_file = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1468a5ed-ab40-466d-b650-466f77a4852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "index_to_word = {}\n",
    "\n",
    "for i,word in enumerate(total_words):\n",
    "    word_to_index[word] = i+1\n",
    "    index_to_word[i+1] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30cea145-fb2e-4411-b728-ebb78a4ff78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word[1846] = 'startseq'\n",
    "word_to_index['startseq'] = 1846\n",
    "index_to_word[1847] = 'endseq'\n",
    "word_to_index['endseq'] = 1847\n",
    "vocab_size = len(word_to_index) + 1\n",
    "#print(\"Vocab Size\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6607ceb-67f1-4d1f-bd6e-8fe2c765b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for key in train_descriptions.keys():\n",
    "    for cap in train_descriptions[key]:\n",
    "        max_len = max(max_len,len(cap.split()))\n",
    "\n",
    "#print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d77587c2-39dc-458b-a205-289455b41ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_descriptions,encoding_train_file,word_to_index,max_len,vocab_size,batch_size):\n",
    "    X1,X2,Y = [],[],[]\n",
    "    n = 0\n",
    "    \n",
    "    while True:\n",
    "        for key,desc_list in train_descriptions.items():\n",
    "            n += 1\n",
    "\n",
    "            photo = encoding_train_file[key]\n",
    "            for desc in desc_list:\n",
    "                \n",
    "                seq = [word_to_index[word] for word in desc.split() if word in word_to_index]\n",
    "                for i in range(1,len(seq)):\n",
    "                    xi = seq[0:i]\n",
    "                    yi = seq[i]\n",
    "                    xi = pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n",
    "                    yi = to_categorical([yi],num_classes=vocab_size)[0]\n",
    "                    X1.append(photo)\n",
    "                    X2.append(xi)\n",
    "                    Y.append(yi)\n",
    "                    \n",
    "                if n==batch_size:\n",
    "                    yield[[np.array(X1),np.array(X2)],np.array(Y)]\n",
    "                    X1,X2,Y = [],[],[]\n",
    "                    n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daf6488e-1a69-4d6b-bb8b-22da62eb78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Data/glove.6B.50d.txt\",encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90f9e249-6480-4f0f-b42a-11cf0d23677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    word_embedding = np.array(values[1:],dtype = 'float')\n",
    "    embedding_index[word] = word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd516a57-b0e9-431f-b1e9-229d15bc1d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95fe0399-d3c2-401f-92ec-3ea9b691041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix():\n",
    "    emb_dim = 50\n",
    "    matrix = np.zeros((vocab_size,emb_dim))\n",
    "    for word,index in word_to_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            matrix[index] = embedding_vector\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a1eb44a-4063-45dc-b6c1-5717e11eac2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1848, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = get_embedding_matrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e8ff1de-b425-48d5-a353-41795f1cd308",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_features = Input(shape=(2048,))\n",
    "input_img1 = Dropout(0.3)(input_img_features)\n",
    "input_img2 = Dense(256,activation='relu')(input_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87fdaaff-0088-442a-9205-baebf944c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_captions = Input(shape=(max_len,))\n",
    "input_cap1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\n",
    "input_cap2 = Dropout(0.3)(input_cap1)\n",
    "input_cap3 = LSTM(256)(input_cap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23377efa-77ad-4c3c-acdc-c8e929e87ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1 = add([input_img2,input_cap3])\n",
    "decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size,activation='softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs=[input_img_features,input_captions],outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8222d56-cde5-4366-bb84-11f421b735fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75cbd6-0829-4a24-8fe1-91efa44e0a12",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e690a09f-8c75-469d-9436-07787e2dd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 2\n",
    "steps = len(train_descriptions)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f7842b6-d5ce-4c34-b8cd-431a1dfdfd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(epochs):\\n    generator = data_generator(train_descriptions, encoding_train_file, word_to_index, max_len, vocab_size, batch_size)\\n    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, encoding_train_file, word_to_index, max_len, vocab_size, batch_size)\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50449734-47f8-4424-8ee1-e6416f576887",
   "metadata": {},
   "source": [
    "### To save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c67d0367-ebcc-4431-bac0-fa48665a928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('./model_weights/model_test(50epochs)'+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5084bf-d17d-4e3d-b283-cd45eb18c94a",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d6a6493-da9e-488a-971a-30ae38502ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_caption(photo):\n",
    "    model_load = load_model('/Study Related all DATA/Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/model_weights/model_test(50epochs).h5')\n",
    "    in_text = \"startseq\"\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        sequence = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_len, padding='post')\n",
    "\n",
    "        ypred =  model_load.predict([photo,sequence])\n",
    "        ypred = ypred.argmax()\n",
    "        word = index_to_word[ypred]\n",
    "        in_text += (' ' + word)\n",
    "        \n",
    "        if word =='endseq':\n",
    "            break\n",
    "        \n",
    "        \n",
    "    final_caption = in_text.split()\n",
    "    final_caption = final_caption[1:-1]\n",
    "    final_caption = ' '.join(final_caption)\n",
    "    \n",
    "    return final_caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d6cd113-8b54-4742-bfc9-a0110b60586f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(5):\\n    rn =  np.random.randint(0, 1000)\\n\\n    img_name = list(encoding_test_file.keys())[rn]\\n    photo = encoding_test_file[img_name].reshape((1,2048))\\n\\n    i = plt.imread(IMG_PATH +\"/\" +img_name+\".jpg\")\\n    plt.imshow(i)\\n    plt.axis(\"off\")\\n    plt.show()\\n\\n    caption = predict_caption(photo)\\n    print(caption)\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(5):\n",
    "    rn =  np.random.randint(0, 1000)\n",
    "\n",
    "    img_name = list(encoding_test_file.keys())[rn]\n",
    "    photo = encoding_test_file[img_name].reshape((1,2048))\n",
    "\n",
    "    i = plt.imread(IMG_PATH +\"/\" +img_name+\".jpg\")\n",
    "    plt.imshow(i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    caption = predict_caption(photo)\n",
    "    print(caption)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fa82a75-6991-491f-8289-0843a2834f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "\n",
    "def open_image():\n",
    "    global file_path\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        filetypes=[(\"Image files\", \"*.jpeg *.jpg *.png\")]\n",
    "    )\n",
    "    if file_path:\n",
    "        img = Image.open(file_path)\n",
    "        img.thumbnail((800, 800))\n",
    "        img = ImageTk.PhotoImage(img)\n",
    "        label.config(image=img)\n",
    "        label.image = img\n",
    "\n",
    "\n",
    "def generate_caption():\n",
    "    global file_path\n",
    "    if not hasattr(label, 'image'):\n",
    "        messagebox.showerror(\"Error: Image Not Found\", \"Uh-oh! We can't proceed without an image. Please select an image. then click 'Generate Caption'.\")\n",
    "        return\n",
    "\n",
    "    photo = encode_image(file_path)\n",
    "    photo = np.reshape(photo, (1, 2048))\n",
    "    caption = predict_caption(photo)\n",
    "    caption_var.set(\"Caption: \" + caption)\n",
    "    \n",
    "\n",
    "root = tk.Tk()\n",
    "root.geometry(\"1350x900\")\n",
    "root.resizable(0, 0)\n",
    "root.title(\"CAPTION CRAFT: ADVANCED AI IMAGE CAPTIONING SYSTEM\")\n",
    "\n",
    "project_img = Image.open(\"/Study Related all DATA\\Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Main/AI_Logo.png\")\n",
    "project_img = project_img.resize((60, 60))\n",
    "project_img = ImageTk.PhotoImage(project_img)\n",
    "project_label = tk.Label(root, image=project_img)\n",
    "project_label.pack(padx=0, pady=0)\n",
    "\n",
    "heading_label = tk.Label(root, text=\"CAPTION CRAFT: ADVANCED AI IMAGE CAPTIONING SYSTEM\", font=(\"Helvetica\", 14, \"bold\"))\n",
    "heading_label.pack(padx=0, pady=0)\n",
    "\n",
    "predefined_img = Image.open(\"/Study Related all DATA\\Major Project (MCA_NEW)/Jupyter Notebook/Major Project (Caption Craft)/Main/No_Image_Available.jpg\")\n",
    "predefined_img.thumbnail((500, 500))\n",
    "predefined_img = ImageTk.PhotoImage(predefined_img)\n",
    "label = tk.Label(root, image=predefined_img)\n",
    "label.pack(padx=10, pady=10)\n",
    "\n",
    "\n",
    "button = tk.Button(root, text=\"Open Image\", command=open_image, height=2, width=30, font=(\"Helvetica\", 15, \"bold\"), borderwidth=4, relief=\"raised\")\n",
    "button.pack(padx=10, pady=5)\n",
    "\n",
    "button_generate = tk.Button(root, text=\"Generate Caption\", command=generate_caption, height=2, width=30, font=(\"Helvetica\", 15, \"bold\"), borderwidth=4, relief=\"raised\")\n",
    "button_generate.pack(padx=10, pady=5)\n",
    "\n",
    "caption_var = tk.StringVar()\n",
    "caption_label = tk.Label(root, textvariable=caption_var, font=(\"Helvetica\", 18, \"italic\"), bg=\"lightgrey\")\n",
    "caption_label.pack(padx=10, pady=20)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4045f-fff1-4c63-980c-6a0c8bee6ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
